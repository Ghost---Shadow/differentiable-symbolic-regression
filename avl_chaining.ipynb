{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf.print('-'*80)\n",
    "# tf.print(r1)\n",
    "# tf.print(tape.gradient(r1, k1))\n",
    "# tf.print(tape.gradient(loss_1, k1))\n",
    "# tf.print('-'*80)\n",
    "# tf.print(tape.gradient(r1, k2))\n",
    "# tf.print(tape.gradient(loss_1, k2))\n",
    "\n",
    "# tf.print('-'*80)\n",
    "# tf.print(r2)\n",
    "# tf.print(loss_2)\n",
    "# tf.print(tape.gradient(r2, k1))\n",
    "# tf.print(tape.gradient(loss_2, k1))\n",
    "# tf.print('-'*80)\n",
    "# tf.print(tape.gradient(r2, k2))\n",
    "# tf.print(tape.gradient(loss_2, k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "[1]\n",
      "[[1 20]]\n",
      "--------------------------------------------------------------------------------\n",
      "[upstream_grads] [-9.]\n",
      "[forward_result] [1.]\n",
      "[target] [10.]\n",
      "[d_idx] [0]\n",
      "[k_grad] [-1.  1.]\n",
      "[clipped_abs_grad] 1.0\n",
      "[final] [-1.  1.]\n",
      "[v_grad] [1. 0.]\n",
      "--------------------------------------------------------------------------------\n",
      "[upstream_grads] [-9. -0.]\n",
      "[forward_result] [ 1. 20.]\n",
      "[target] [10. 20.]\n",
      "[d_idx] [1 1]\n",
      "[k_grad] [[ 1. -1.  1.]\n",
      " [ 1. -1.  1.]]\n",
      "[clipped_abs_grad] [1. 0.]\n",
      "[final] [[ 1. -1.  1.]\n",
      " [ 0. -0.  0.]]\n",
      "[v_grad] [[1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "--------------------------------------------------------------------------------\n",
      "[[-1 1]]\n"
     ]
    }
   ],
   "source": [
    "# @tf.function\n",
    "@tf.custom_gradient\n",
    "def asymmetrical_vectored_lookup(v, k):\n",
    "    tf.debugging.assert_rank(v, 2)\n",
    "    tf.debugging.assert_rank(k, 2)\n",
    "    tf.debugging.assert_equal(tf.shape(v), tf.shape(k))\n",
    "\n",
    "    k_shape = tf.shape(k)\n",
    "\n",
    "    # Pick the value at the most likely index, non-differentiably\n",
    "    b_idx = tf.argmax(k, axis=-1)\n",
    "    idx_len = tf.shape(b_idx)[0]\n",
    "    a_idx = tf.range(idx_len, dtype=tf.int64)\n",
    "    idx = tf.stack([a_idx, b_idx], axis=1)\n",
    "    forward_result = tf.gather_nd(v, idx)\n",
    "\n",
    "    def grad(upstream_grads):\n",
    "        tf.print(f'[upstream_grads] {upstream_grads}')\n",
    "        tf.print(f'[forward_result] {forward_result}')\n",
    "        # Estimate the target scalar which we want to look up\n",
    "        target = forward_result - upstream_grads\n",
    "        tf.print(f'[target] {target}')\n",
    "        target = tf.expand_dims(target, -1)\n",
    "\n",
    "        # Find the index of element in the array which is closest to target\n",
    "        diff_vector = tf.math.squared_difference(v, target)\n",
    "        d_idx = tf.argmin(diff_vector, axis=-1)\n",
    "        tf.print(f'[d_idx] {d_idx}')\n",
    "\n",
    "        # Create a vector which is 1 everywhere except the idx\n",
    "        # of the target, where it is -1\n",
    "        ones = tf.ones(k_shape)\n",
    "        eyes = tf.one_hot([d_idx], k_shape[-1])[0]\n",
    "        k_grad = -(2 * eyes - ones)\n",
    "\n",
    "        # d/dv (v . k) = k\n",
    "        v_grad = k\n",
    "\n",
    "        upstream_grads = tf.expand_dims(upstream_grads, -1)\n",
    "\n",
    "        # 1. The k_grad should dictate the direction of the vector. So, upstream grad is always positive\n",
    "        # 2. We want it to scale to zero as it gets closer to target. So we clip it between 0 and 1.\n",
    "        # 3. If there is an exact match in the vector, then we dont send the gradients downstream for other entries. (disabled)\n",
    "        min_clipped_abs_grad = tf.abs(upstream_grads)\n",
    "        min_clipped_abs_grad = tf.clip_by_value(min_clipped_abs_grad, 0, 1)\n",
    "        # min_clipped_abs_grad = tf.reduce_min(min_clipped_abs_grad)\n",
    "        tf.print(f'[k_grad] {tf.squeeze(k_grad)}')\n",
    "        tf.print(f'[clipped_abs_grad] {tf.squeeze(min_clipped_abs_grad)}')\n",
    "        tf.print(f'[final] {tf.squeeze(min_clipped_abs_grad * k_grad )}')\n",
    "        tf.print(f'[v_grad] {tf.squeeze(v_grad)}')\n",
    "\n",
    "        tf.print('-'*80)\n",
    "        return upstream_grads * v_grad, min_clipped_abs_grad * k_grad \n",
    "\n",
    "    return forward_result, grad\n",
    "\n",
    "# v = tf.constant([[1,2,3], [10,20,30]], dtype=tf.float32)\n",
    "v = tf.constant([[1,11,3], [12,20,30]], dtype=tf.float32)\n",
    "k1 = tf.constant([[1,0,0], [0,1,0]], dtype=tf.float32)\n",
    "k2 = tf.constant([[1,0]], dtype=tf.float32)\n",
    "t1 = tf.constant([2, 20], dtype=tf.float32)\n",
    "t2 = tf.constant([10], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(k1)\n",
    "    tape.watch(k2)\n",
    "\n",
    "    r1 = asymmetrical_vectored_lookup(v, k1)\n",
    "    loss_1 = tf.nn.l2_loss(r1 - t1)\n",
    "\n",
    "    r1 = tf.expand_dims(r1, 0)\n",
    "\n",
    "    r2 = asymmetrical_vectored_lookup(r1, k2)\n",
    "    loss_2 = tf.nn.l2_loss(r2 - t2)\n",
    "\n",
    "tf.print('-'*80)\n",
    "tf.print(r2)\n",
    "tf.print(r1)\n",
    "tf.print('-'*80)\n",
    "# tf.print(tape.gradient(loss_2, v))\n",
    "# tf.print(tape.gradient(loss_2, k1))\n",
    "# tf.print(tape.gradient(loss_1, k1))\n",
    "# tf.print('-'*80)\n",
    "tf.print(tape.gradient(loss_2, k2))\n",
    "# tf.print('-'*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36 144]\n",
      "[upstream] [1. 1.]\n",
      "[upstream] [12. 24.]\n",
      "[36 96]\n",
      "[36 96]\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def bar(x, y):\n",
    "  def grad(upstream):\n",
    "    tf.print(f'[upstream] {upstream}')\n",
    "    dz_dx = y\n",
    "    dz_dy = x\n",
    "    return upstream * dz_dx, upstream * dz_dy\n",
    "  z = x * y\n",
    "  return z, grad\n",
    "x = tf.constant([2.0,3.0], dtype=tf.float32)\n",
    "y = tf.constant([3.0,4.0], dtype=tf.float32)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  tape.watch(y)\n",
    "  w = bar(x, y)\n",
    "  z = bar(w, w)\n",
    "\n",
    "tf.print(z)\n",
    "tf.print(tape.gradient(z, x))\n",
    "# tf.print(tape.gradient(z, y))\n",
    "tf.print(2*x*y*y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1d83f9e8f5c8837e86d6304fd35fdd4149e22cdc219dfc99f4444b7e631426b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
