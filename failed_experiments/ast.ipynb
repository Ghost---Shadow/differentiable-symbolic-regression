{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0 0 0]\n",
      "0 [1 0 0]\n",
      "0.707106769 [0.707106829 -0.707106709 0]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def is_phi(element):\n",
    "    tf.debugging.assert_rank(element, 1)\n",
    "    \n",
    "    elem_dim = tf.shape(element)[0]\n",
    "    phi = tf.one_hot(0, elem_dim)\n",
    "    \n",
    "    element = tf.math.l2_normalize(element)\n",
    "    t = tf.tensordot(element, phi, axes=1)\n",
    "\n",
    "    return t\n",
    "\n",
    "test1 = tf.Variable([1,0,0], dtype=tf.float32)\n",
    "test2 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "test3 = tf.Variable([.5,.5,0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    result1 = is_phi(test1)\n",
    "    result2 = is_phi(test2)\n",
    "    result3 = is_phi(test3)\n",
    "\n",
    "tf.print(result1, tape.gradient(result1, test1))\n",
    "tf.print(result2, tape.gradient(result2, test2))\n",
    "tf.print(result3, tape.gradient(result3, test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.stacks import stack_push, stack_pop, stack_peek, new_stack, new_stack_from_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_ S O T x + '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_pretty_print(tokens):\n",
    "    tokens = tf.argmax(tokens, axis=1)\n",
    "    lookup = ['_', 'S', 'O', 'T', 'x', '+']\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    for token in tokens:\n",
    "        result += f'{lookup[token]} '\n",
    "        \n",
    "    return result\n",
    "\n",
    "tokens = tf.transpose(tf.one_hot([0,1,2,3,4,5], 6, dtype=tf.float32))\n",
    "tokens_pretty_print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0 1 0]\n",
      " [0.0428932235 0.0428932235 0.707106769]\n",
      " [0 0.707106769 0.0857864469]]\n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[0.707106769 0.292893231 0]\n",
      "[1 0 0]\n",
      "[0.0606601834 0.792893231 0.792893231]\n",
      "([[0 0 0]\n",
      " [0.207106784 0.207106784 0.207106784]\n",
      " [0.207106799 0.207106799 0.207106799]], [2.87867975 1.53553391 1.76776707])\n"
     ]
    }
   ],
   "source": [
    "# @tf.function\n",
    "def safe_push(stack, element, is_phi_fn):\n",
    "    tf.debugging.assert_rank_at_least(stack[0], 2)\n",
    "    tf.debugging.assert_rank(stack[1], 1)\n",
    "    tf.debugging.assert_equal(tf.shape(stack[0])[1:], tf.shape(element))\n",
    "    tf.debugging.assert_equal(tf.rank(stack[0]) - 1, tf.rank(element) )\n",
    "    \n",
    "    t = is_phi_fn(element)\n",
    "    \n",
    "    old_buffer, old_index = stack\n",
    "    new_buffer, new_index = stack_push(stack, element)\n",
    "\n",
    "    buffer = t * old_buffer + (1 - t) * new_buffer\n",
    "    index = t * old_index + (1 - t) * new_index\n",
    "    \n",
    "#     tf.print(tokens_pretty_print(old_buffer))\n",
    "#     tf.print(tokens_pretty_print(new_buffer))\n",
    "#     tf.print(tokens_pretty_print(buffer))\n",
    "#     tf.print('-'*80)\n",
    "\n",
    "    # Hack to tell tensorflow that the shape has not changed\n",
    "    # TODO: Why does this hack work?\n",
    "    buffer = tf.reshape(buffer, tf.shape(old_buffer))\n",
    "    index = tf.reshape(index, tf.shape(old_index))\n",
    "\n",
    "    new_stack = (buffer, index)\n",
    "\n",
    "    return new_stack\n",
    "\n",
    "stack = new_stack((3,3), True)\n",
    "original_stack = stack\n",
    "\n",
    "element1 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "element2 = tf.Variable([0.5,0.5,0], dtype=tf.float32)\n",
    "element3 = tf.Variable([0,0,1], dtype=tf.float32)\n",
    "element4 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack = safe_push(stack, element1, is_phi)\n",
    "    stack = safe_push(stack, element2, is_phi)\n",
    "    stack = safe_push(stack, element3, is_phi)\n",
    "    stack = safe_push(stack, element4, is_phi)\n",
    "    \n",
    "tf.print(stack[0])\n",
    "tf.print(tf.round(stack[0]))\n",
    "tf.print(stack[1])\n",
    "tf.print(tf.round(stack[1]))\n",
    "tf.print(tape.gradient(stack[0], element3))\n",
    "tf.print(tape.gradient(stack, original_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[1 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]], [0 0 1])\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def pop_and_purge(stack, phi):\n",
    "    stack_len = tf.shape(stack[0])[1]\n",
    "    stack, element = stack_pop(stack)\n",
    "    stack = stack_push(stack, phi)\n",
    "    stack, _ = stack_pop(stack)\n",
    "    \n",
    "    return stack, element\n",
    "\n",
    "stack = new_stack_from_buffer(tf.ones((3,3), dtype=tf.float32))\n",
    "phi = tf.one_hot(0, 3, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack, element = pop_and_purge(stack, phi)\n",
    "    \n",
    "tf.print(stack)\n",
    "tf.print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function stack_push at 0x00000278249BCB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function stack_push at 0x00000278249BCB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function stack_push at 0x00000278249BCB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function stack_push at 0x00000278249BCB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[1 0 0]\n",
      "[-1 1 1]\n",
      "([[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]], [4 1 1])\n"
     ]
    }
   ],
   "source": [
    "stack = new_stack((3,3), True)\n",
    "\n",
    "element1 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "element2 = tf.Variable([1,0,0], dtype=tf.float32)\n",
    "element3 = tf.Variable([0,0,1], dtype=tf.float32)\n",
    "element4 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "\n",
    "original_stack = stack\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack = safe_push(stack, element1, is_phi)\n",
    "    stack = safe_push(stack, element2, is_phi)\n",
    "    stack = safe_push(stack, element3, is_phi)\n",
    "    stack = safe_push(stack, element4, is_phi)\n",
    "    \n",
    "tf.print(stack[0])\n",
    "tf.print(stack[1])\n",
    "tf.print(tape.gradient(stack[0], element3))\n",
    "tf.print(tape.gradient(stack, original_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.array_ops import tensor_lookup_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_DIM = 6\n",
    "PRODUCTION_DIM = 4\n",
    "STACK_SIZE = 10\n",
    "PHI = np.eye(TOKEN_DIM)[0]\n",
    "S = np.eye(TOKEN_DIM)[1]\n",
    "O = np.eye(TOKEN_DIM)[2]\n",
    "T = np.eye(TOKEN_DIM)[3]\n",
    "X = np.eye(TOKEN_DIM)[4]\n",
    "PLUS = np.eye(TOKEN_DIM)[5]\n",
    "\n",
    "E = [PHI, PHI, PHI]\n",
    "\n",
    "G_s = tf.constant([\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, [S, O, T], E, E, E, E],\n",
    "    [E, [T, PHI, PHI], E, E, E, E],\n",
    "], dtype=tf.float32)\n",
    "G_o = tf.constant([\n",
    "    [E, E, E, [X, PHI, PHI], E, E],\n",
    "    [E, E, [PLUS, PHI, PHI], E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "], dtype=tf.float32)\n",
    "grammar = (G_s, G_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function is_phi at 0x000002787F9B0B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function stack_push at 0x00000278249BCB88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "T O S _ _ _ _ _ _ _ \n",
      "[-0.0833333358 -0.0833333358 3 -0.0416666679]\n"
     ]
    }
   ],
   "source": [
    "# @tf.function\n",
    "def production_step(grammar, production, stack, output, phi, is_phi_fn):\n",
    "    tf.debugging.assert_rank(grammar[0], 4)\n",
    "    tf.debugging.assert_rank(grammar[1], 4)\n",
    "    tf.debugging.assert_rank(production, 1)\n",
    "    tf.debugging.assert_rank(stack[0], 2)\n",
    "    tf.debugging.assert_rank(output[0], 2)\n",
    "    \n",
    "    G_s, G_o = grammar\n",
    "    \n",
    "    # Save the shapes\n",
    "    stack_0_shape = tf.shape(stack[0])\n",
    "    stack_1_shape = tf.shape(stack[1])\n",
    "    output_0_shape = tf.shape(output[0])\n",
    "    output_1_shape = tf.shape(output[1])\n",
    "    \n",
    "    # Get next token from stack\n",
    "    stack, stack_top_token = pop_and_purge(stack, phi)\n",
    "\n",
    "    # Push tokens back onto the stack\n",
    "    tokens_to_push = tensor_lookup_2d(G_s, production, stack_top_token)\n",
    "    for token in tf.reverse(tokens_to_push, axis=[0]):\n",
    "        stack = safe_push(stack, token, is_phi_fn)\n",
    "    \n",
    "    # Push tokens to output\n",
    "    tokens_to_push = tensor_lookup_2d(G_o, production, stack_top_token)\n",
    "    for token in tokens_to_push:\n",
    "        output = safe_push(output, token, is_phi_fn)\n",
    "    \n",
    "    return stack, output\n",
    "\n",
    "stack = new_stack(((STACK_SIZE, TOKEN_DIM)))\n",
    "output = new_stack(((STACK_SIZE, TOKEN_DIM)))\n",
    "\n",
    "stack = safe_push(stack, tf.constant(S, dtype=tf.float32), is_phi)\n",
    "production = tf.one_hot(2, PRODUCTION_DIM)\n",
    "phi = tf.one_hot(0, TOKEN_DIM, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(grammar)\n",
    "    tape.watch(production)\n",
    "    tape.watch(stack)\n",
    "    tape.watch(output)\n",
    "    \n",
    "    new_s, new_o = production_step(grammar, production, stack, output, phi, is_phi)\n",
    "\n",
    "tf.print(tokens_pretty_print(new_s[0]))\n",
    "# tf.print(tape.gradient(new_o, output))\n",
    "# tf.print(tape.gradient(new_s, stack))\n",
    "# tf.print(tape.gradient(new_s[0], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_s[1], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_o[0], grammar[1]).shape)\n",
    "# tf.print(tape.gradient(new_o[1], grammar[1]).shape)\n",
    "tf.print(tape.gradient(new_s, production))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-1f0d1a89a940>:14: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n",
      "S _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "\n",
    "phi = tf.one_hot([0], TOKEN_DIM, dtype=tf.float32)\n",
    "soft_phi = tf.nn.softmax(phi, axis=-1)\n",
    "stack_buffer = tf.tile(soft_phi, (stack_shape[0], 1))\n",
    "soft_stack = new_stack_from_buffer(stack_buffer)\n",
    "output_buffer = tf.tile(soft_phi, (stack_shape[0], 1))\n",
    "soft_output = new_stack_from_buffer(output_buffer)\n",
    "soft_s = tf.nn.softmax(tf.constant(S, dtype=tf.float32))\n",
    "\n",
    "soft_stack = safe_push(soft_stack, soft_s, is_phi)\n",
    "soft_p = tf.nn.softmax(tf.one_hot(2, PRODUCTION_DIM))\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(grammar)\n",
    "    tape.watch(production)\n",
    "    tape.watch(stack)\n",
    "    tape.watch(output)\n",
    "    \n",
    "    # Soften the grammar\n",
    "    gs, go = grammar\n",
    "    sgs, sgo = tf.nn.softmax(gs), tf.nn.softmax(go)\n",
    "    soft_g = (sgs, sgo)\n",
    "\n",
    "    new_s, new_o = production_step(soft_g, soft_p, soft_stack, soft_output, soft_phi[0], is_phi)\n",
    "# tf.config.experimental_run_functions_eagerly(False)\n",
    "\n",
    "tf.print(tokens_pretty_print(soft_stack[0]))\n",
    "#here\n",
    "tf.print(tokens_pretty_print(new_s[0]))\n",
    "# tf.print(tape.gradient(new_o, output))\n",
    "# tf.print(tape.gradient(new_s, stack))\n",
    "# tf.print(tape.gradient(new_s[0], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_s[1], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_o[0], grammar[1]).shape)\n",
    "# tf.print(tape.gradient(new_o[1], grammar[1]).shape)\n",
    "tf.print(tape.gradient(new_s, production))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_step_info(grammar, production, stack, output):\n",
    "    gs, go = grammar\n",
    "    top = stack_peek(stack)\n",
    "    tf.print('p\\t', tf.argmax(production))\n",
    "    i = tf.argmax(production)\n",
    "    j = tf.argmax(top)\n",
    "    tf.print('G_s\\t', tokens_pretty_print(gs[i][j]), (i,j))\n",
    "    tf.print('G_o\\t', tokens_pretty_print(go[i][j]), (i,j))\n",
    "    tf.print('S_i+1\\t', tokens_pretty_print(stack[0]), tf.argmax(stack[1]))\n",
    "    tf.print('O_i+1\\t', tokens_pretty_print(output[0]), tf.argmax(output[1]))\n",
    "    tf.print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generate(grammar, productions, stack_shape, S, phi, is_phi_fn, print_steps=False):\n",
    "    # Reserve space for stack and output\n",
    "    stack_buffer = tf.tile(phi, (stack_shape[0], 1))\n",
    "    stack = new_stack_from_buffer(stack_buffer)\n",
    "    output_buffer = tf.tile(phi, (stack_shape[0], 1))\n",
    "    output = new_stack_from_buffer(output_buffer)\n",
    "    \n",
    "    # Push S to top of stack\n",
    "    stack = safe_push(stack, S, is_phi)\n",
    "    \n",
    "    productions = tf.unstack(productions)\n",
    "\n",
    "    for production in productions:\n",
    "        stack, output = production_step(grammar, production, stack, output, phi[0], is_phi_fn)\n",
    "        \n",
    "        if print_steps:\n",
    "            dump_step_info(grammar, production, stack, output)\n",
    "        \n",
    "    return tf.reverse(output[0], axis=[0]), stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\t 2\n",
      "G_s\t S O T  (2, 1)\n",
      "G_o\t _ _ _  (2, 1)\n",
      "S_i+1\t T O S _ _ _ _ _ _ _  3\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 3\n",
      "G_s\t _ _ _  (3, 3)\n",
      "G_o\t _ _ _  (3, 3)\n",
      "S_i+1\t T O T _ _ _ _ _ _ _  3\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 2)\n",
      "G_o\t _ _ _  (0, 2)\n",
      "S_i+1\t T O _ _ _ _ _ _ _ _  2\n",
      "O_i+1\t x _ _ _ _ _ _ _ _ _  1\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 1\n",
      "G_s\t _ _ _  (1, 3)\n",
      "G_o\t _ _ _  (1, 3)\n",
      "S_i+1\t T _ _ _ _ _ _ _ _ _  1\n",
      "O_i+1\t x + _ _ _ _ _ _ _ _  2\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 0)\n",
      "G_o\t _ _ _  (0, 0)\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "O_i+1\t x + x _ _ _ _ _ _ _  3\n",
      "--------------------------------------------------------------------------------\n",
      "Final result:\n",
      "_ _ _ _ _ _ _ x + x \n",
      "--------------------------------------------------------------------------------\n",
      "Final stack:\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "[[-1.5138166 -1.5138166 3 -1.09121823]\n",
      " [-0.237847239 -0.237847239 0.00173611124 1]\n",
      " [1 0.0416666679 0.0416666679 0.0416666679]\n",
      " [0.0416666679 1 0.0416666679 0.0416666679]\n",
      " [1 0.0416666679 0.0416666679 0.0416666679]]\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "productions = tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM)\n",
    "\n",
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "d_S = tf.constant(S, dtype=tf.float32)\n",
    "d_phi = tf.constant(tf.one_hot([0], TOKEN_DIM))\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(productions)\n",
    "    output, final_stack = generate(grammar, productions, stack_shape, d_S, d_phi, is_phi, True)\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "tf.print('Final result:')\n",
    "tf.print(tokens_pretty_print(output))\n",
    "tf.print('-'*80)\n",
    "tf.print('Final stack:')\n",
    "tf.print(tokens_pretty_print(final_stack))\n",
    "tf.print('-'*80)\n",
    "tf.print(tape.gradient(output, productions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_to_tokens(s, token_dim, total_length):\n",
    "    lookup = ['_', 'S', 'O', 'T', 'x', '+']\n",
    "    arr = []\n",
    "    for t in s.split(' '):\n",
    "        arr.append(lookup.index(t))\n",
    "    \n",
    "    phi = lookup.index('_')\n",
    "    arr = ([phi] * (total_length - len(arr))) + arr\n",
    "        \n",
    "    return tf.one_hot(arr, token_dim)\n",
    "\n",
    "encode_to_tokens('x + x +', TOKEN_DIM, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\t 2\n",
      "G_s\t _ _ _  (2, 0)\n",
      "G_o\t _ _ _  (2, 0)\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  1\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 3\n",
      "G_s\t _ _ _  (3, 0)\n",
      "G_o\t _ _ _  (3, 0)\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  1\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 0)\n",
      "G_o\t _ _ _  (0, 0)\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  2\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 1\n",
      "G_s\t _ _ _  (1, 0)\n",
      "G_o\t _ _ _  (1, 0)\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  2\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 0)\n",
      "G_o\t _ _ _  (0, 0)\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  9\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  3\n",
      "--------------------------------------------------------------------------------\n",
      "Final result:\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "Final stack:\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "33.2177277\n",
      "[[0.000342945335 0.000342945335 -0.00102883601 0.000342945335]\n",
      " [0.000698299438 0.000698299438 0.000698299438 -0.00209489814]\n",
      " [-0.0044236877 0.00147456292 0.00147456292 0.00147456292]\n",
      " [0.00259305956 -0.00777917728 0.00259305956 0.00259305956]\n",
      " [-0.0118159363 0.00393864559 0.00393864559 0.00393864559]]\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "productions = tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM)\n",
    "\n",
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "d_S = tf.constant(S, dtype=tf.float32)\n",
    "d_phi = tf.constant(tf.one_hot([0], TOKEN_DIM))\n",
    "expected_output = encode_to_tokens('x + x +', TOKEN_DIM, STACK_SIZE)\n",
    "zero_stack = tf.one_hot([0] * stack_shape[0], stack_shape[1], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(productions)\n",
    "    # Soften the grammar\n",
    "    gs, go = grammar\n",
    "    sgs, sgo = tf.nn.softmax(gs), tf.nn.softmax(go)\n",
    "    soft_g = (sgs, sgo)\n",
    "\n",
    "    # Soften the productions\n",
    "    soft_p = tf.nn.softmax(productions,axis=-1)\n",
    "\n",
    "    # Soften S\n",
    "    soft_s = tf.nn.softmax(d_S)\n",
    "\n",
    "    soft_phi = tf.nn.softmax(d_phi, axis=-1)\n",
    "    \n",
    "    output_, stack_ = generate(soft_g, soft_p, stack_shape, soft_s, soft_phi, is_phi, True)\n",
    "\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(output, output_))\n",
    "    loss += tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(zero_stack, stack_))\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "tf.print('Final result:')\n",
    "tf.print(tokens_pretty_print(output_))\n",
    "tf.print('-'*80)\n",
    "tf.print('Final stack:')\n",
    "tf.print(tokens_pretty_print(stack_))\n",
    "tf.print('-'*80)\n",
    "tf.print(loss)\n",
    "tf.print(tape.gradient(loss, productions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\t 2\n",
      "G_s\t S O T  (2, 1)\n",
      "G_o\t _ _ _  (2, 1)\n",
      "S_i+1\t T O S _ _ _ _ _ _ _  3\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 3\n",
      "G_s\t _ _ _  (3, 3)\n",
      "G_o\t _ _ _  (3, 3)\n",
      "S_i+1\t T O T _ _ _ _ _ _ _  3\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 2)\n",
      "G_o\t _ _ _  (0, 2)\n",
      "S_i+1\t T O _ _ _ _ _ _ _ _  2\n",
      "O_i+1\t x _ _ _ _ _ _ _ _ _  1\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 1\n",
      "G_s\t _ _ _  (1, 3)\n",
      "G_o\t _ _ _  (1, 3)\n",
      "S_i+1\t T _ _ _ _ _ _ _ _ _  1\n",
      "O_i+1\t x + _ _ _ _ _ _ _ _  2\n",
      "--------------------------------------------------------------------------------\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 0)\n",
      "G_o\t _ _ _  (0, 0)\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "O_i+1\t x + x _ _ _ _ _ _ _  3\n",
      "--------------------------------------------------------------------------------\n",
      "0\n",
      "_ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ x + x \n",
      "_ _ _ _ _ _ _ _ _ _ \n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-1)\n",
    "\n",
    "@tf.function\n",
    "def train_step(grammar, productions, stack_shape, S, is_phi_fn, output, print_steps=False):\n",
    "    zero_stack = tf.one_hot([0] * stack_shape[0], stack_shape[1], dtype=tf.float32)\n",
    "    phi = tf.one_hot([0], stack_shape[1], dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(productions)\n",
    "#         # Soften the grammar\n",
    "#         gs, go = grammar\n",
    "#         sgs, sgo = tf.nn.softmax(gs), tf.nn.softmax(go)\n",
    "#         soft_g = (sgs, sgo)\n",
    "        \n",
    "#         # Soften the productions\n",
    "#         soft_p = tf.nn.softmax(productions,axis=-1)\n",
    "        \n",
    "#         # Soften S\n",
    "#         soft_s = tf.nn.softmax(S)\n",
    "        \n",
    "#         soft_phi = tf.nn.softmax(phi, axis=-1)\n",
    "        \n",
    "#         output_, stack_ = generate(soft_g, soft_p, stack_shape, soft_s, soft_phi, is_phi_fn, True)\n",
    "        \n",
    "#         loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(output, output_))\n",
    "#         loss += tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(zero_stack, stack_))\n",
    "        \n",
    "        output_, stack_ = generate(grammar, productions, stack_shape, S, phi, is_phi_fn, print_steps)\n",
    "        loss = tf.nn.l2_loss(output - output_) + tf.nn.l2_loss(zero_stack - stack_)\n",
    "        \n",
    "    grads = tape.gradient(loss, productions)\n",
    "    opt.apply_gradients(zip([grads], [productions]))\n",
    "    \n",
    "    return loss, output_, stack_\n",
    "\n",
    "MAX_PRODUCTIONS = 5\n",
    "# productions = tf.Variable(tf.one_hot([0] * MAX_PRODUCTIONS, PRODUCTION_DIM), dtype=tf.float32)\n",
    "productions = tf.Variable(tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM, dtype=tf.float32))\n",
    "# productions = tf.Variable(tf.one_hot([2, 0, 0, 0, 0], PRODUCTION_DIM), dtype=tf.float32)\n",
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "d_S = tf.constant(S, dtype=tf.float32)\n",
    "output = encode_to_tokens('x + x', TOKEN_DIM, STACK_SIZE)\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "loss, output_, stack_ = train_step(grammar, productions, stack_shape, d_S, is_phi, output, True)\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "tf.print(loss)\n",
    "tf.print(tokens_pretty_print(output_), tokens_pretty_print(output))\n",
    "tf.print(tokens_pretty_print(stack_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 _ _ _ _ _ _ _ _ x x  _ _ _ _ _ _ _ _ _ _  [2 3 0 0 0]\n",
      "1.56940734 _ _ _ _ _ _ _ _ _ x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "0.0560943037 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "0.0634588823 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "0.0014611911 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "0.00997392554 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "0.000278108782 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "0.000933527073 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "0.000314138335 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n",
      "3.93871378e-05 _ _ _ _ _ _ _ x + x  _ _ _ _ _ _ _ _ _ _  [2 3 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# productions = tf.Variable(tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM), dtype=tf.float32)\n",
    "productions = tf.Variable(tf.one_hot([2, 3, 0, 0, 0], PRODUCTION_DIM), dtype=tf.float32)\n",
    "output = encode_to_tokens('x + x', TOKEN_DIM, STACK_SIZE)\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "for var in opt.variables():\n",
    "    var.assign(tf.zeros_like(var))\n",
    "\n",
    "for i in range(100):\n",
    "    loss, output_, stack_ = train_step(grammar, productions, stack_shape, d_S, is_phi, output)\n",
    "    if i % 10 == 0:\n",
    "        p_output = tokens_pretty_print(output_.numpy())\n",
    "        p_stack = tokens_pretty_print(stack_.numpy())\n",
    "        \n",
    "        tf.print(loss, p_output, p_stack, tf.argmax(productions, axis=-1))\n",
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
