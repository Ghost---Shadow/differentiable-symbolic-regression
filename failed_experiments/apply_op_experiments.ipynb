{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit238183a89ccd4c25acc508071275f29e",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(4.4, shape=(), dtype=float32)\ntf.Tensor(4.8, shape=(), dtype=float32)\ntf.Tensor(4.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "def foo2(x, y):\n",
    "    a = x * x\n",
    "    b = x * x * x\n",
    "    \n",
    "    return y * b + (1 - y) * a\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(0.1)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = foo2(x, y)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, x))\n",
    "print(tape.gradient(z, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(4.0, shape=(), dtype=float32)\ntf.Tensor(1.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def foo(x, y):\n",
    "    def grad(g):\n",
    "        if y > 0.5:\n",
    "            dy_dx = 2 * x\n",
    "        else:\n",
    "            dy_dx = 3 * x * x\n",
    "\n",
    "        return dy_dx * g, g\n",
    "    if y > 0.5:\n",
    "        return x * x, grad\n",
    "    else:\n",
    "        return x * x * x, grad\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(0.6)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = foo(x, y)\n",
    "\n",
    "print(tape.gradient(z, x))\n",
    "print(tape.gradient(z, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.99996841 3.16217775e-05]\n[0.999683917 0.000316127785]\n[0.5 0.5]\n[0.787298381 0.212701619]\nWARNING:tensorflow:5 out of the last 5 calls to <function to_prob_dist at 0x00000204CBE9B168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n[0.666666687 0.333333343]\nWARNING:tensorflow:6 out of the last 6 calls to <function to_prob_dist at 0x00000204CBE9B168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n--------------------------------------------------------------------------------\n[[0.75 0.25]\n [0.875 0.125]\n [0.5 0.5]\n [0.99996841 3.16217775e-05]]\n[[0.03125 -0.09375]\n [0.0117187509 -0.08203125]\n [0 0]\n [3.15904617e-05 -0]]\n"
    }
   ],
   "source": [
    "@tf.function\n",
    "def to_prob_dist(v):\n",
    "    v2 = tf.sqrt(tf.square(v)+1e-9)\n",
    "    # v2 = tf.sqrt(tf.square(v))\n",
    "    m = tf.expand_dims(tf.reduce_sum(v2, axis=-1),-1)\n",
    "    n = tf.math.divide_no_nan(v2, m)\n",
    "    return n\n",
    "\n",
    "tf.print(to_prob_dist([1.0, 0.0]))\n",
    "tf.print(to_prob_dist([0.1, 0.0]))\n",
    "tf.print(to_prob_dist([0.1, 0.1]))\n",
    "tf.print(to_prob_dist([78.1, 21.1]))\n",
    "tf.print(to_prob_dist([2.0, 1.0]))\n",
    "x = tf.Variable([\n",
    "        [3.0, 1.0],\n",
    "        [7.0, 1.0],\n",
    "        [1.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "    ])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = to_prob_dist(x)\n",
    "    loss = tf.nn.l2_loss(z)\n",
    "print('-'*80)\n",
    "tf.print(z)\n",
    "tf.print(tape.gradient(loss, x))\n",
    "# tf.print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.99996841 3.16217775e-05]\n[0.999683917 0.000316127785]\n[0.5 0.5]\n[0.787298381 0.212701619]\n[0.666666687 0.333333343]\n--------------------------------------------------------------------------------\n[[0.199999586 0.0666665286]\n [0.466665685 0.0666665286]\n [0.0666665286 0.0666665286]\n [0.0666665286 2.10818075e-06]]\n[[-0.00533327274 -0.0142221246]\n [0.01244443 -0.0142221246]\n [-0.0142221246 -0.0142221246]\n [-0.0142221246 -0]]\n"
    }
   ],
   "source": [
    "@tf.function\n",
    "def to_prob_dist_all(v):\n",
    "    v2 = tf.sqrt(tf.square(v)+1e-9)\n",
    "    # v2 = tf.sqrt(tf.square(v))\n",
    "    m = tf.expand_dims(tf.reduce_sum(v2),-1)\n",
    "    n = tf.math.divide_no_nan(v2, m)\n",
    "    return n\n",
    "\n",
    "tf.print(to_prob_dist([1.0, 0.0]))\n",
    "tf.print(to_prob_dist([0.1, 0.0]))\n",
    "tf.print(to_prob_dist([0.1, 0.1]))\n",
    "tf.print(to_prob_dist([78.1, 21.1]))\n",
    "tf.print(to_prob_dist([2.0, 1.0]))\n",
    "x = tf.Variable([\n",
    "        [3.0, 1.0],\n",
    "        [7.0, 1.0],\n",
    "        [1.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "    ])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = to_prob_dist_all(x)\n",
    "    loss = tf.nn.l2_loss(z)\n",
    "print('-'*80)\n",
    "tf.print(z)\n",
    "tf.print(tape.gradient(loss, x))\n",
    "# tf.print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:6 out of the last 11 calls to <function to_prob_dist at 0x00000204CBE9B168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3 2\ntf.Tensor(2.0, shape=(), dtype=float32)\ntf.Tensor(0.0, shape=(), dtype=float32)\ntf.Tensor([ 4. -4.], shape=(2,), dtype=float32)\n"
    }
   ],
   "source": [
    "def foo3(x, y, op):\n",
    "    a = x + y\n",
    "    b = x - y\n",
    "\n",
    "    op = to_prob_dist(op)\n",
    "    res = tf.stack([a, b])\n",
    "    c = tf.tensordot(res, op, 1)\n",
    "\n",
    "    return c\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(2.0)\n",
    "op = tf.Variable([0.5, 0.5])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = foo3(x, y, op)\n",
    "    loss = tf.nn.l2_loss(z - 1.0)\n",
    "\n",
    "tf.print(z, loss)\n",
    "print(tape.gradient(loss, x))\n",
    "print(tape.gradient(loss, y))\n",
    "print(tape.gradient(loss, op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:6 out of the last 11 calls to <function to_prob_dist at 0x00000204CBE9B168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n0.888889074 2.86666656 [0.190348491 0.809651494] 2.04444456 2.33333349\n7.91181665e-05 2.8198216 [0.061551556 0.938448429] 2.06532216 0.987420797\n5.18369419e-08 2.81931615 [0.0597038157 0.940296173] 2.06576061 0.999678\n3.18962634e-11 2.81930304 [0.0596567206 0.940343261] 2.06577206 0.999992\n1.77635684e-15 2.81930304 [0.0596555285 0.940344512] 2.06577253 0.99999994\n7.10542736e-15 2.81930304 [0.0596555136 0.940344512] 2.06577253 1.00000012\n7.10542736e-15 2.81930304 [0.0596555434 0.940344512] 2.06577253 0.999999881\n0 2.81930304 [0.0596555285 0.940344512] 2.06577253 1\n7.10542736e-15 2.81930304 [0.0596555136 0.940344512] 2.06577253 1.00000012\n7.10542736e-15 2.81930304 [0.0596555434 0.940344512] 2.06577253 0.999999881\n"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(1e-1)\n",
    "# opt = tf.keras.optimizers.Adam(3e-4)\n",
    "target = tf.constant(1.0)\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(2.0)\n",
    "op = tf.Variable([0.5, 1.0])\n",
    "\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = foo3(x, y, op)\n",
    "        loss = tf.nn.l2_loss(z - target)\n",
    "    variables = [x, y, op]\n",
    "    grads = tape.gradient(loss,variables)\n",
    "    opt.apply_gradients(zip(grads, variables))\n",
    "\n",
    "    op.assign(to_prob_dist(op))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        tf.print(loss, x, op, y, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[3 3]\n2.5\n"
    }
   ],
   "source": [
    "@tf.function\n",
    "def dot(a, b):\n",
    "    return tf.reduce_sum(tf.multiply(a, b), axis=-1)\n",
    "\n",
    "a = [2.0, 3.0]\n",
    "b = [[0.0, 1.0],\n",
    " [0.0, 1.0]]\n",
    "tf.print(dot(a,b))\n",
    "tf.print(dot([2.0, 3.0], [0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-0.999746859 1.9994936\ntf.Tensor(\n[[6.341934e-05 0.000000e+00]\n [0.000000e+00 6.341934e-05]], shape=(2, 2), dtype=float32)\ntf.Tensor([0.         0.00037932], shape=(2,), dtype=float32)\n"
    }
   ],
   "source": [
    "@tf.function\n",
    "def foo5(v, s, op):\n",
    "    s = to_prob_dist(s)\n",
    "    op = to_prob_dist(op)\n",
    "\n",
    "    xy = dot(v,s)\n",
    "    x = xy[0]\n",
    "    y = xy[1]\n",
    "\n",
    "    a = x + y\n",
    "    b = x - y\n",
    "    res = tf.stack([a, b])\n",
    "    c = dot(res, op)\n",
    "\n",
    "    return c\n",
    "\n",
    "v = tf.constant([2.0, 3.0])\n",
    "s = tf.Variable([\n",
    "    [1.0, 0.0],\n",
    "    [0.0, 1.0],\n",
    "])\n",
    "op = tf.Variable([0.0, 1.0])\n",
    "target = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = foo5(v, s, op)\n",
    "    loss = tf.nn.l2_loss(z - target)\n",
    "\n",
    "tf.print(z, loss)\n",
    "# print(tape.gradient(z, s))\n",
    "print(tape.gradient(loss, s))\n",
    "print(tape.gradient(loss, op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.5 1.125\ntf.Tensor(\n[[-0.75  0.75]\n [ 0.    0.  ]], shape=(2, 2), dtype=float32)\ntf.Tensor([ 3.75 -3.75], shape=(2,), dtype=float32)\n"
    }
   ],
   "source": [
    "v = tf.constant([2.0, 3.0])\n",
    "s = tf.Variable([\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5],\n",
    "])\n",
    "# s = tf.Variable([\n",
    "#     [0.0, 1.0],\n",
    "#     [1.0, 0.0],\n",
    "# ])\n",
    "op = tf.Variable([0.5, 0.5])\n",
    "target = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = foo5(v, s, op)\n",
    "    loss = tf.nn.l2_loss(z - target)\n",
    "\n",
    "tf.print(z, loss)\n",
    "# print(tape.gradient(z, s))\n",
    "print(tape.gradient(loss, s))\n",
    "print(tape.gradient(loss, op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0 1]\n[[0 1]\n [1 0]]\n"
    }
   ],
   "source": [
    "def collapse_prob(v):\n",
    "    v = tf.convert_to_tensor(v, dtype=tf.float32)\n",
    "\n",
    "    if tf.rank(v) == 1:\n",
    "        v = tf.expand_dims(v, 0)\n",
    "    arr = []\n",
    "    for vv in v:\n",
    "        a = tf.argmax(vv)\n",
    "        s = tf.shape(vv)[0]\n",
    "        arr.append(tf.eye(s, dtype=tf.float32)[a])\n",
    "    return tf.squeeze(tf.stack(arr))\n",
    "\n",
    "tf.print(collapse_prob([0.4,0.6]))\n",
    "tf.print(collapse_prob(tf.Variable([\n",
    "    [0.1, 0.2],\n",
    "    [0.2, 0.1],\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.125 2.5 0.000126489118 5 0 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n7.10542736e-15 1.00000012 -0.999746859 2 3 [0 1]\n"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(1e-2)\n",
    "# opt = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "v = tf.constant([2.0, 3.0])\n",
    "s = tf.Variable([\n",
    "    [0.5, 0.5],\n",
    "    [0.5, 0.5],\n",
    "])\n",
    "op = tf.Variable([0.5, 0.5])\n",
    "target = tf.constant(1.0)\n",
    "\n",
    "for i in range(1000):\n",
    "    op.assign(to_prob_dist(op))\n",
    "    s.assign(to_prob_dist(s))\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = foo5(v, s, op)\n",
    "        loss = tf.nn.l2_loss(z - target)\n",
    "    variables = [s, op]\n",
    "    grads = tape.gradient(loss,variables)\n",
    "    opt.apply_gradients(zip(grads, variables))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        c_s = collapse_prob(s)\n",
    "        c_op = collapse_prob(op)\n",
    "        xy = tf.tensordot(v, c_s, 1)\n",
    "        c_z = foo5(v, c_s, c_op)\n",
    "        tf.print(loss, z, c_z, xy[0], xy[1], c_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ops_matrix(v):\n",
    "    v_s = tf.shape(v)\n",
    "    v_t = tf.tile(v, [v_s[0]])\n",
    "    v_t = tf.reshape(v_t, [v_s[0],-1])\n",
    "\n",
    "    op1 = tf.expand_dims(v_t + tf.expand_dims(v,-1), 0)\n",
    "    op2 = tf.expand_dims(v_t - tf.expand_dims(v,-1), 0)\n",
    "    ops = tf.concat([op1, op2], axis=0)\n",
    "\n",
    "    return ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.500094891 0.124952562\n[[-1.57877803e-05 0], [0.124964438 -0.124980226]]\n[0 6.32405281e-05]\n"
    }
   ],
   "source": [
    "@tf.function\n",
    "def foo6(a, b, op, ops):\n",
    "    s = tf.tensordot(a, b, 0)\n",
    "    s = to_prob_dist_all(s)\n",
    "    op = to_prob_dist_all(op)\n",
    "\n",
    "    pair_choose = tf.reduce_sum(ops * s, axis=[1,2])\n",
    "    op_choose = tf.reduce_sum(pair_choose * op)\n",
    "\n",
    "    return op_choose\n",
    "\n",
    "v = tf.constant([2.0, 3.0])\n",
    "a = tf.Variable([1.0, 0.0])\n",
    "b = tf.Variable([1.0, 1.0])\n",
    "op = tf.Variable([0.0, 1.0])\n",
    "target = tf.constant(1.0)\n",
    "ops = gen_ops_matrix(v)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = foo6(a, b, op, ops)\n",
    "    loss = tf.nn.l2_loss(z - target)\n",
    "\n",
    "tf.print(z, loss)\n",
    "# print(tape.gradient(z, s))\n",
    "tf.print(tape.gradient(loss, [a, b]))\n",
    "tf.print(tape.gradient(loss, op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.125 2.5 [50 50] [51 49] [46 54]\n0.0523815304 1.32367122 [49 51] [54 46] [26 74]\n0.0013753731 1.05244756 [49 51] [55 45] [22 78]\n3.04003443e-05 1.00779748 [49 51] [55 45] [21 79]\n6.52791414e-07 1.00114262 [49 51] [55 45] [21 79]\n1.39664564e-08 1.00016713 [49 51] [55 45] [21 79]\n2.98605585e-10 1.00002444 [49 51] [55 45] [21 79]\n6.39488462e-12 1.00000358 [49 51] [55 45] [21 79]\n2.55795385e-13 1.00000072 [49 51] [55 45] [21 79]\n7.10542736e-15 1.00000012 [49 51] [55 45] [21 79]\n"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.SGD(1e-2)\n",
    "# opt = tf.keras.optimizers.Adam(3e-4)\n",
    "\n",
    "v = tf.constant([2.0, 3.0])\n",
    "a = tf.Variable([0.5, 0.5])\n",
    "b = tf.Variable([0.5, 0.5])\n",
    "op = tf.Variable([0.5, 0.5])\n",
    "target = tf.constant(1.0)\n",
    "ops = gen_ops_matrix(v)\n",
    "\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = foo6(a, b, op, ops)\n",
    "        loss = tf.nn.l2_loss(z - target)\n",
    "\n",
    "    variables = [a, b, op]\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    opt.apply_gradients(zip(grads, variables))\n",
    "    op.assign(to_prob_dist_all(op))\n",
    "    a.assign(to_prob_dist_all(a))\n",
    "    b.assign(to_prob_dist_all(b))\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        tf.print(loss, z, tf.round(a*100), tf.round(b*100), tf.round(op*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}